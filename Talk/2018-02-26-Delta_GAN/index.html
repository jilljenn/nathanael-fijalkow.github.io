<!doctype html>

<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=1024" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <title>Introduction</title>
    
    <meta name="description" content="Intro" />
    <meta name="author" content="Nathana&euml;l Fijalkow" />

    <link href="http://fonts.googleapis.com/css?family=Open+Sans:regular,semibold,italic,italicsemibold|PT+Sans:400,700,400italic,700italic|PT+Serif:400,700,400italic,700italic" rel="stylesheet" />

    <link href="css/style.css" rel="stylesheet" />
</head>

<body>

<div id="flides">

    <div id="overview_start" class="step" data-x="0" data-y="0" data-scale="3">
    </div>

    <div id="name" class="step" data-fixed="1" data-duration="0" data-x="0" data-y="-800">
	<h2>Nathana&euml;l Fijalkow</h2>
	<h3>CNRS, LaBRI, Bordeaux, and The Alan Turing Institute of data science, London</h3>
    </div>






    <div id="title_intro" class="step" data-fixed="1" data-duration="0" data-x="-1000" data-y="-500">
	<h2>Introduction</h2>
    </div>

    <div id="setting" class="step" data-x="-1300" data-y="-300" data-scale=".6">
	<h3>Image <red>classifier</red></h3>

	$$f : \textit{Images} \to \left\{\textit{Cat},\textit{Dog}\right\}$$

	<br/>
	For mathematical purposes:
	$$\textit{Images} = \mathbb{R}^d$$
	$$\textit{Cat} = 0 \text{ and } \textit{Dog} = 1$$
    </div>

    <div id="network" class="step" data-x="-1300" data-y="0" data-scale=".6">
	<h3>Neural network</h3>

	<center><img id="gate" src="img/gate.svg"/></center>	
    </div>

    <div id="architecture" class="step" data-x="-700" data-y="-300" data-scale=".6">
	<h3>Architecture</h3>

	<center><img id="architecture" src="img/neural_network.svg"/></center>	
    </div>

    <div id="training" class="step" data-x="-700" data-y="0" data-scale=".6">

	<center><img id="setting" src="img/learning.svg"/></center>	
	<br/>
	<h3>Training reduces to an optimisation problem</h3>
	<br/>
	Standard techniques include: <red>(stochastic) gradient descent</red>

	<blue>Important</blue>: may fail!
    </div>





    <div id="overview_1" class="step" data-x="0" data-y="0" data-scale="3">
    </div>




    <div id="title_theory" class="step" data-fixed="1" data-duration="0" data-x="1000" data-y="-500">
	<h2>Theory</h2>
    </div>

    <div id="overfitting" class="step" data-x="800" data-y="-300" data-scale=".6">
	<h3>Overfitting</h3>
	<br/>
	Arora, 2017, <a href="http://www.offconvex.org/2017/12/08/generalization1/">offconvex.org</a>:
	<br/><br/>
	<center>Lately many ML theorists have become interested in the generalization mystery: 
	<blue>why do trained deep nets perform well on previously unseen data</blue>, even though they have way more free parameters than the number of datapoints <br/>
	(<red>the classic ''overfitting'' regime</red>)?</center>
    </div>

    <div id="remark" class="step" data-x="800" data-y="0" data-scale=".6">
	<h3>A remark</h3>

	<br/><br/>
	<blue>Machine learning</blue> is driven by <red>empirical studies</red>, and <green>theory</green> lags behind...
	<br/><br/>
	<blue>Automata</blue> is driven by <green>theory</green>, and <red>empirical studies</red> lag behind...
    </div>

    <div id="VC" class="step" data-x="1400" data-y="-300" data-scale=".6">
	<h3>Theoretical notions</h3>

	<br/>
	<blue>VC dimension</blue> quantifies the ratio between number of samples and parameters
	<br/><br/>
	<b>Theorem</b> (Jerrum, McIntyre, Koiran, up until this year...)
	<br/>
	VC dimension of neural networks with $w$ parameters
	<ul>
		<li>threshold: $O(w^2)$ and $\Omega(w^2)$
		<li>sigmoid: $O(w^4)$
		<li>identity (roughly, automata): $O(w)$ and $\Omega(w)$
	</ul>
    </div>

    <div id="questions" class="step" data-x="1400" data-y="0" data-scale=".6">
	<h3>Questions</h3>
	<br/>
	Neural networks with strange activation functions are hard to analyse...
	<br/><br/>
	What about <red>automata</red>?
	How do they perform <blue>experimentally</blue>?
	<br/><br/>
	What are the right <green>theoretical</green> notions to <green>assist the design</green> of machine learning engines?
    </div>





    <div id="overview_2" class="step" data-x="0" data-y="0" data-scale="3">
    </div>








    <div id="title_gan" class="step" data-fixed="1" data-duration="0" data-x="0" data-y="0">
	<h2>Generative Adversarial Networks</h2>
    </div>

    <div id="generative" class="step" data-x="-1400" data-y="600" data-scale=".6">
	<h3>Generative model</h3>

	$$f : \textit{Random Seeds} \to \textit{Cat Images}$$

	<br/>
	For mathematical purposes:
	$$\textit{Random Seeds} = \mathbb{R}^d$$
	$$\textit{Cat Images} \subset \mathbb{R}^d$$
    </div>

    <div id="openai" class="step" data-x="-800" data-y="600" data-scale=".6">
	<h3>OpenAI.com</h3>

	<center><img id="openai" src="img/gen_models_anim_2.gif"/></center>	
    </div>

    <div id="perplexity" class="step" data-x="-300" data-y="600" data-scale=".6">
	<h3>Perplexity</h3>

	<br/>
	Problem: what is $\textit{Cat Images}$?
	<br/>
	<h3><red>How to measure the progress<br/> of a generative model?</red></h3>
	<br/>
	<div class="slide_step">
	<b>GAN idea</b>: a model is good if it is hard to <green>distinguish</green> from an actual distribution of cat images
	</div>	
	<br/>
	<div class="slide_step">
	We introduce a neural net in charge of <green>discriminating</green>
	</div>	

    </div>

    <div id="art" class="step" data-x="200" data-y="600" data-scale=".6">
	<b>Definition</b>: 
	$$\textit{Artist} : \textit{Inspiration} \to \textit{Images}$$
	$$\textit{Good Artist} : \textit{Inspiration} \to \textit{Beautiful Images}$$
		
	<div class="slide_step">
	An art teacher can generate $\textit{Beautiful Images}$ through a distribution $\textit{Art}$
	</div>	

	<div class="slide_step">
	<br/>
	<b>Idea</b>:
	<ul>
		<li>Train a <blue>teacher</blue> to distinguish $\textit{Artist}$ from $\textit{Art}$
		<li>Train the <green>student</green> to get good grades
	</ul>
	<red>Hopefully</red> if the teacher fails, then the student became a good artist! 
	</div>	

    </div>

    <div id="experiments" class="step" data-x="850" data-y="600" data-scale=".6">
	<h3>Experiments (TensorFlow, Keras) on MNIST</h3>
	<br/>
	<ul>
		<li>works <red>amazingly well</red> if you have a good GAN architecture!
		<li>works <green>reasonably well</green> with automata
	</ul>

	<br/><br/>
	<center>There <blue>almost always</blue> exists a <blue>good</blue> equilibrium!</center>

	<div class="slide_step">
	<h2>Why?</h2>
	</div>
    </div>

    <div id="questions_final" class="step" data-x="1450" data-y="600" data-scale=".6">
	<h3>Questions</h3>

	Arora et al in <a href="https://arxiv.org/abs/1703.00573">ICML'17</a> showed:
	<ul>
		<li>there may not be an equilibrium
		<li>there may be (only) bad equilibria
	</ul>

	<div class="slide_step">
	<br/><br/>
	<h3>What about <green>automata</green>?</h3>
	<ul>
		<li>existence of an equilibrium?
		<li>how to compute this equilibrium?
		<li>what is the right distance between distributions for training?
	</ul>

	</div>
		
    </div>




    <div id="overview" class="step" data-x="0" data-y="0" data-scale="3">
    </div>


</div>

<script src="js/flides.js"></script>
<script>flides().init();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
